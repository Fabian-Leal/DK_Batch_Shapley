{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basis functions for all points\n",
    "\n",
    "Block size:\n",
    "\n",
    "chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/http://www.saimm.org.za/Journal/v106n03p205.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dk_model import DeepKrigingTrainer\n",
    "import numpy as np\n",
    "import gstools as gs\n",
    "\n",
    "import scipy.stats as stats\n",
    "##Library neural nets\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.style.use(\"seaborn\")\n",
    "import matplotlib.pyplot as plt\n",
    "get_ipython().magic(u'matplotlib inline')\n",
    "get_ipython().magic(u\"config InlineBackend.figure_format = 'svg'\")\n",
    "import matplotlib;matplotlib.rcParams['figure.figsize'] = (8,6)\n",
    "import pylab\n",
    "\n",
    "deposit_data = pd.read_csv(\"Data/final_dataset_1.csv\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = deposit_data['X'].min(), deposit_data['X'].max()   \n",
    "y_min, y_max = deposit_data['Y'].min(), deposit_data['Y'].max()   \n",
    "z_min, z_max = deposit_data['Z'].min(), deposit_data['Z'].max()   \n",
    "\n",
    "x = np.linspace(x_min, x_max, 25)\n",
    "y = np.linspace(y_min, y_max, 25)\n",
    "z = np.linspace(z_min, z_max, 25)\n",
    "\n",
    "X, Y, Z = np.meshgrid(x, y, z)\n",
    "\n",
    "grid_points = np.vstack([X.ravel(), Y.ravel(), Z.ravel()]).T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(grid_points)\n",
    "\n",
    "lon = grid_points[:, 0]\n",
    "lat = grid_points[:, 1]\n",
    "az = grid_points[:, 2]\n",
    "\n",
    "num_basis_1_lvl = [10**3]\n",
    "\n",
    "num_basis_list = [num_basis_1_lvl]\n",
    "\n",
    "phi_arrays = []  \n",
    "\n",
    "# For each grid\n",
    "for grid in num_basis_list:\n",
    "    knots_1dx = [np.linspace(0, 1, int(i**(1/3)) + 1) for i in grid]\n",
    "    knots_1dy = [np.linspace(0, 1, int(i**(1/3)) + 1) for i in grid]\n",
    "    knots_1dz = [np.linspace(0, 1, int(i**(1/3)) + 1) for i in grid]\n",
    "    basis_size = 0\n",
    "    phis = np.zeros((N, sum(grid)))\n",
    "    \n",
    "    # For each level of resolution\n",
    "    for res in range(len(grid)):\n",
    "        theta = 1 / (grid[res]**(1/3)) * 2.5\n",
    "        knots_x, knots_y, knots_z = np.meshgrid(knots_1dx[res], knots_1dy[res], knots_1dz[res])\n",
    "        knots = np.column_stack((knots_x.flatten(), knots_y.flatten(), knots_z.flatten()))\n",
    "        \n",
    "        # For each node in the grid\n",
    "        for i in range(grid[res]):\n",
    "            d = np.linalg.norm(np.vstack((lon, lat, az)).astype(float).T - knots[i, :], axis=1) / theta\n",
    "            \n",
    "            # For each distance of our data to the node i, calculate Wendland kernel\n",
    "            for j in range(len(d)):\n",
    "                if 0 <= d[j] <= 1:\n",
    "                    phis[j, i + basis_size] = (1 - d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3) / 3\n",
    "                else:\n",
    "                    phis[j, i + basis_size] = 0\n",
    "        \n",
    "        basis_size += grid[res]\n",
    "    \n",
    "    phi_arrays.append(phis) \n",
    "\n",
    "phi_1_lvl = phi_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_1_lvl_df = pd.DataFrame(phi_1_lvl[0])\n",
    "\n",
    "# Remove all-zero columns\n",
    "#phi_1_lvl_df = phi_1_lvl_df.loc[:, (phi_1_lvl_df != 0).any(axis=0)]\n",
    "\n",
    "new_columns = {i: f'phi_{i}' for i in range(phi_1_lvl_df.shape[1])}\n",
    "\n",
    "phi_1_lvl_df.columns = pd.Index(new_columns.values())\n",
    "\n",
    "xyz_df = pd.DataFrame(grid_points)\n",
    "xyz_df.columns = ['X', 'Y', 'Z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = np.array(deposit_data['Density_gcm3']).reshape(-1,1)\n",
    "phi = np.array(deposit_data.iloc[:, 10:])\n",
    "N = len(deposit_data)\n",
    "indices = np.arange(N)\n",
    "Xs = np.array(deposit_data.iloc[:, 1:4])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "indices = np.arange(N)\n",
    "Xs_train, Xs_test, phi_train, phi_test, y_train, y_test, idx_train, idx_test     = train_test_split(Xs, phi, y, indices, test_size=0.2)\n",
    "N_train = Xs_train.shape[0]\n",
    "N_test = Xs_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_pred = np.array(xyz_df)\n",
    "phi_pred = np.array(phi_1_lvl_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_points_df= pd.DataFrame(grid_points)\n",
    "grid_points_df[['X','Y','Z']] = pd.DataFrame(grid_points)\n",
    "grid_points_df = grid_points_df[['X','Y','Z']] \n",
    "\n",
    "filtered_unnormalised_deposit_data = pd.read_csv('Data/filtered_unnormalised_deposit_data.csv')\n",
    "covariates=['X', 'Y', 'Z', 'CP_Total', 'PO_Total', 'PY_Total', 'RQD_Pct', 'Cr_ppm']\n",
    "# Calculate the multiplier for each column\n",
    "multipliers = {}\n",
    "for column in covariates + ['Density_gcm3']:\n",
    "    ratio = deposit_data[column] / filtered_unnormalised_deposit_data[column]\n",
    "    multiplier = ratio.mean()\n",
    "    multipliers[column] = 1/multiplier\n",
    "\n",
    "multipliers_df = pd.DataFrame.from_dict(multipliers, orient='index', columns=['Multiplier'])\n",
    "\n",
    "multipliers['X'] = filtered_unnormalised_deposit_data['X'].max() - filtered_unnormalised_deposit_data['X'].min()\n",
    "multipliers['Y'] = filtered_unnormalised_deposit_data['Y'].max() - filtered_unnormalised_deposit_data['Y'].min()\n",
    "multipliers['Z'] = filtered_unnormalised_deposit_data['Z'].max() - filtered_unnormalised_deposit_data['Z'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout,BatchNormalization\n",
    "import tensorflow as tf\n",
    "# seed = 42\n",
    "# tf.random.set_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "\n",
    "def create_mlp(feature_dim):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, input_dim = feature_dim,  kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dropout(rate=0.2))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='poisson', optimizer='adam', metrics=['mse','mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = create_mlp(feature_dim = 3)\n",
    "print(\"[INFO] training model nn...\")\n",
    "train_history = model_nn.fit(Xs_train, y_train, validation_split = 0.2, epochs = 500, batch_size = 32, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dk = create_mlp(feature_dim = 1000)\n",
    "print(\"[INFO] training model dk...\")\n",
    "train_history_dk = model_dk.fit(phi_train, y_train, validation_split = 0.2, epochs = 500, batch_size = 32, verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_test_pred = model_nn.predict(Xs_test)\n",
    "mse = np.mean((nn_test_pred - y_test) ** 2)\n",
    "\n",
    "dk_test_pred = model_dk.predict(phi_test)\n",
    "mse_dk = np.mean((dk_test_pred - y_test) ** 2)\n",
    "\n",
    "\n",
    "print(f\"MSE: {mse:.4f}, {mse_dk:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = model_dk.predict(phi_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpnn = model_nn.predict(Xs_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "\n",
    "\n",
    "# # Use GSTools for Ordinary Kriging\n",
    "# model = gs.Gaussian(dim=3)\n",
    "# krige = gs.krige.Ordinary(model, cond_pos=[Xs_train[:, 0], Xs_train[:, 1], Xs_train[:, 2]], cond_val=y_train, fit_variogram=True)\n",
    "\n",
    "# # Execute kriging at the points of xyz_df\n",
    "# kriging_pred, kriging_var = krige([xyz_df['X'].values, xyz_df['Y'].values, xyz_df['Z'].values], return_var=True )\n",
    "\n",
    "# # Display kriging predictions\n",
    "# print(kriging_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict(model, x, n_iter=100):\n",
    "    preds = []\n",
    "    for i in range(n_iter):\n",
    "        y_p = model(x, training=True) # enable dropout\n",
    "        preds.append(y_p.numpy())\n",
    "    preds = np.array(preds)\n",
    "    mean_preds = preds.mean(axis=0)\n",
    "    std_preds = preds.std(axis=0)\n",
    "    return mean_preds, std_preds\n",
    "\n",
    "mean_preds, std_preds = mc_dropout_predict(model_dk, phi_pred, n_iter=100)\n",
    "mean_preds_nn, std_preds_nn = mc_dropout_predict(model_nn, Xs_pred, n_iter=100)\n",
    "\n",
    "print(\"Predicted means:\", mean_preds.flatten())\n",
    "print(\"Predicted uncertainties (std):\", std_preds.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_csv('Data/3d_grid_interpolations_var.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "krig_std = np.sqrt(combined_data['Kriging_Var'].values)\n",
    "plt.hist(krig_std, bins=300, edgecolor='black')\n",
    "plt.title('Histogram of std_preds')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(std_preds*multipliers['Density_gcm3'], bins=300, edgecolor='black')\n",
    "plt.title('Histogram of std_preds')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_xyz = grid_points_df[['X', 'Y', 'Z']].values\n",
    "\n",
    "# deep_kriging_df = pd.DataFrame(\n",
    "#     np.hstack([test_data_xyz, interp.ravel()[:, None]]),\n",
    "#     columns=['X', 'Y', 'Z', 'DeepKriging_Pred']\n",
    "# )\n",
    "\n",
    "# nn_df = pd.DataFrame(\n",
    "#     np.hstack([test_data_xyz, interpnn.ravel()[:, None]]),\n",
    "#     columns=['X', 'Y', 'Z', 'NN_Pred']\n",
    "# )\n",
    "\n",
    "# kriging_df = pd.DataFrame(\n",
    "#     np.hstack([test_data_xyz, kriging_pred.ravel()[:, None], kriging_var.ravel()[:, None]]),\n",
    "#     columns=['X', 'Y', 'Z', 'Kriging_Pred', 'Kriging_Var']\n",
    "# )\n",
    "\n",
    "# combined_data = deep_kriging_df.merge(nn_df, on=['X', 'Y', 'Z'], how='inner') \\\n",
    "#                                .merge(kriging_df, on=['X', 'Y', 'Z'], how='inner')\n",
    "\n",
    "# combined_data[['DeepKriging_Pred', 'NN_Pred', 'Kriging_Pred']] *= multipliers['Density_gcm3']\n",
    "\n",
    "# combined_data['Kriging_Var'] *= multipliers['Density_gcm3'] ** 2\n",
    "\n",
    "# # **Add standard deviations to combined_data**\n",
    "# combined_data['DeepKriging_Std'] = std_preds * multipliers['Density_gcm3']\n",
    "# combined_data['NN_Std'] = std_preds_nn * multipliers['Density_gcm3']\n",
    "\n",
    "# file_path = \"3d_grid_interpolations_var.csv\"\n",
    "# combined_data.to_csv(file_path, index=False)\n",
    "\n",
    "# print(f\"Data saved to: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data['DeepKriging_Std'] = std_preds * multipliers['Density_gcm3']\n",
    "combined_data['NN_Std'] = std_preds_nn * multipliers['Density_gcm3']\n",
    "\n",
    "combined_data['DeepKriging_Pred'] = mean_preds*multipliers['Density_gcm3']\n",
    "combined_data['NN_Pred'] = mean_preds_nn*multipliers['Density_gcm3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_data['DeepKriging_Pred'] = interp*multipliers['Density_gcm3']\n",
    "# combined_data['NN_Pred'] = interpnn*multipliers['Density_gcm3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_middle = combined_data['X'].median()\n",
    "y_middle = combined_data['Y'].median()\n",
    "\n",
    "xz_plane_data = combined_data[combined_data['Y'] == y_middle]\n",
    "\n",
    "yz_plane_data = combined_data[combined_data['X'] == x_middle]\n",
    "\n",
    "z_plane_data = combined_data[combined_data['Z'] == 0]   \n",
    "x_plane_data = combined_data[combined_data['X'] == 0]   \n",
    "y_plane_data = combined_data[combined_data['Y'] == 1.0]\n",
    "\n",
    "plane_data_xzyz = pd.concat([xz_plane_data, yz_plane_data, z_plane_data, x_plane_data, y_plane_data])\n",
    "\n",
    "x_middle = combined_data['X'].median()\n",
    "z_middle = combined_data['Z'].median()\n",
    "\n",
    "xy_plane_data = combined_data[combined_data['Z'] == z_middle]\n",
    "\n",
    "yz_plane_data = combined_data[combined_data['X'] == x_middle]\n",
    "\n",
    "z_plane_data = combined_data[combined_data['Z'] == 0]   \n",
    "x_plane_data = combined_data[combined_data['X'] == 0]   \n",
    "y_plane_data = combined_data[combined_data['Y'] == 1.0] \n",
    "\n",
    "plane_data_xyyz = pd.concat([xy_plane_data, yz_plane_data, z_plane_data, x_plane_data, y_plane_data])\n",
    "\n",
    "y_middle = combined_data['Y'].median()\n",
    "z_middle = combined_data['Z'].median()\n",
    "\n",
    "xy_plane_data = combined_data[combined_data['Z'] == z_middle]\n",
    "\n",
    "xz_plane_data = combined_data[combined_data['Y'] == y_middle]\n",
    "\n",
    "z_plane_data = combined_data[combined_data['Z'] == 0]  \n",
    "x_plane_data = combined_data[combined_data['X'] == 0]   \n",
    "y_plane_data = combined_data[combined_data['Y'] == 1.0] \n",
    "\n",
    "plane_data_xyxz = pd.concat([xy_plane_data, xz_plane_data, z_plane_data, x_plane_data, y_plane_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def filter_outliers(data_xyz, data_values, threshold):\n",
    "    mean = np.mean(data_values)\n",
    "    std_dev = np.std(data_values)\n",
    "    mask = np.abs(data_values - mean) <= threshold * std_dev\n",
    "    filtered_xyz = data_xyz[mask]\n",
    "    filtered_values = data_values[mask]\n",
    "    return filtered_xyz, filtered_values\n",
    "\n",
    "datasets = [plane_data_xzyz, plane_data_xyyz, plane_data_xyxz]\n",
    "\n",
    "colors = [(1, 0, 1), (0, 1, 1)]  # Magenta to Cyan\n",
    "custom_cmap = LinearSegmentedColormap.from_list('MagentaCyan', colors)\n",
    "\n",
    "threshold = 50\n",
    "\n",
    "all_values = []\n",
    "\n",
    "filtered_data = []\n",
    "for data in datasets:\n",
    "    test_data_xyz = data[['X', 'Y', 'Z']].values * [\n",
    "        multipliers['X'], multipliers['Y'], multipliers['Z']\n",
    "    ]\n",
    "    interp = data['DeepKriging_Pred'].values\n",
    "    interpnn = data['NN_Pred'].values\n",
    "    kriging_pred = data['Kriging_Pred'].values\n",
    "\n",
    "    filtered_test_xyz_1, filtered_interp = filter_outliers(test_data_xyz, interp, threshold)\n",
    "    filtered_test_xyz_2, filtered_interpnn = filter_outliers(test_data_xyz, interpnn, threshold)\n",
    "    filtered_test_xyz_3, filtered_kriging = filter_outliers(test_data_xyz, kriging_pred, threshold)\n",
    "\n",
    "    all_values.extend([filtered_interp, filtered_interpnn, filtered_kriging])\n",
    "\n",
    "    filtered_data.append([(filtered_test_xyz_2, filtered_interpnn),\n",
    "                          (filtered_test_xyz_3, filtered_kriging),\n",
    "                          (filtered_test_xyz_1, filtered_interp)])\n",
    "\n",
    "filtered_train_xyz, filtered_y = filter_outliers(\n",
    "    Xs_train * [multipliers['X'], multipliers['Y'], multipliers['Z']],\n",
    "    y_train.ravel(), threshold\n",
    ")\n",
    "\n",
    "vmin, vmax = np.min(all_values), np.max(all_values)\n",
    "\n",
    "fig, axs = plt.subplots(3, 4, figsize=(24, 15), subplot_kw={'projection': '3d'}, \n",
    "                        gridspec_kw={'wspace': 0.4, 'hspace': 0}) \n",
    "\n",
    "for ax_row in axs:\n",
    "    for ax in ax_row:\n",
    "        ax.set_facecolor((0, 0, 0, 0))  \n",
    "\n",
    "\n",
    "titles = ['Training data', 'Deep neural net', 'Ordinary kriging', 'Deepkriging']\n",
    "\n",
    "rotations = [(30, -60), (30, 350), (30, 280)]  \n",
    "for i, (elev, azim) in enumerate(rotations):\n",
    "    axs[i, 0].scatter(\n",
    "        filtered_train_xyz[:, 0], filtered_train_xyz[:, 1], filtered_train_xyz[:, 2],\n",
    "        c=filtered_y * multipliers['Density_gcm3'], cmap=custom_cmap, s=8, vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    axs[i, 0].view_init(elev=elev, azim=azim)  \n",
    "    if i == 0:\n",
    "        axs[i, 0].set_title('Training data', fontsize=12)\n",
    "    axs[i, 0].set_xlabel('Easting [m]', fontsize=8)\n",
    "    axs[i, 0].set_ylabel('Northing [m]', fontsize=8)\n",
    "    axs[i, 0].set_zlabel('Altitude [m]', fontsize=8)\n",
    "    axs[i, 0].tick_params(labelsize=7)\n",
    "    axs[i, 0].grid(False)\n",
    "\n",
    "for i, row_data in enumerate(filtered_data):\n",
    "    for j, (xyz, values) in enumerate(row_data):\n",
    "        axs[i, j + 1].scatter(\n",
    "            xyz[:, 0], xyz[:, 1], xyz[:, 2],\n",
    "            c=values, cmap=custom_cmap, s=28, vmin=vmin, vmax=vmax\n",
    "        )\n",
    "        if i == 0:  \n",
    "            axs[i, j + 1].set_title(titles[j + 1], fontsize=12)\n",
    "        axs[i, j + 1].set_xlabel('Easting [m]', fontsize=8)\n",
    "        axs[i, j + 1].set_ylabel('Northing [m]', fontsize=8)\n",
    "        axs[i, j + 1].set_zlabel('Altitude [m]', fontsize=8)\n",
    "        axs[i, j + 1].tick_params(labelsize=7)\n",
    "        axs[i, j + 1].grid(False)\n",
    "\n",
    "norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "sm = plt.cm.ScalarMappable(cmap=custom_cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "cbar = fig.colorbar(sm, ax=axs, pad=0.05, aspect=50, extend='both', location='right')\n",
    "cbar.set_label('Predicted density (g/cm³)', fontsize=10)\n",
    "cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"Figures/predictions_plot.png\", dpi=300, bbox_inches='tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap, Normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def separate_outliers(data_xyz, data_values, threshold):\n",
    "    mean = np.mean(data_values)\n",
    "    std_dev = np.std(data_values)\n",
    "    mask = np.abs(data_values - mean) <= threshold * std_dev\n",
    "    inliers_xyz = data_xyz[mask]\n",
    "    inliers_values = data_values[mask]\n",
    "    outliers_xyz = data_xyz[~mask]\n",
    "    outliers_values = data_values[~mask]\n",
    "    return inliers_xyz, inliers_values, outliers_xyz, outliers_values\n",
    "\n",
    "datasets = [plane_data_xzyz, plane_data_xyyz, plane_data_xyxz]\n",
    "\n",
    "colors = [(0, 0, 1), (1, 1, 1), (1, 0, 0)]  # Blue -> White -> Red\n",
    "custom_cmap = LinearSegmentedColormap.from_list('BlueWhiteRed', colors)\n",
    "\n",
    "\n",
    "# Threshold for outlier filtering\n",
    "threshold = 50\n",
    "\n",
    "# Initialize variables to store global min and max values\n",
    "all_values = []\n",
    "\n",
    "filtered_data = []\n",
    "for data in datasets:\n",
    "    test_data_xyz = data[['X', 'Y', 'Z']].values * [\n",
    "        multipliers['X'], multipliers['Y'], multipliers['Z']\n",
    "    ]\n",
    "    interp = data['DeepKriging_Std'].values\n",
    "    interpnn = data['NN_Std'].values\n",
    "    kriging_pred = np.sqrt(data['Kriging_Var'].values)\n",
    "\n",
    "    inliers_test_xyz_nn, inliers_interpnn, outliers_test_xyz_nn, outliers_interpnn = separate_outliers(test_data_xyz, interpnn, threshold)\n",
    "\n",
    "    # Keep the full range for DeepKriging and Ordinary Kriging data\n",
    "    filtered_test_xyz_1, filtered_interp = test_data_xyz, interp\n",
    "    filtered_test_xyz_3, filtered_kriging = test_data_xyz, kriging_pred\n",
    "\n",
    "    all_values.extend([filtered_interp, inliers_interpnn, filtered_kriging])\n",
    "\n",
    "    filtered_data.append([\n",
    "        ((inliers_test_xyz_nn, inliers_interpnn), (outliers_test_xyz_nn, outliers_interpnn)),  # NN std (filtered + outliers)\n",
    "        (filtered_test_xyz_3, filtered_kriging),   # Ordinary Kriging std (unfiltered)\n",
    "        (filtered_test_xyz_1, filtered_interp)     # DeepKriging std (unfiltered)\n",
    "    ])\n",
    "\n",
    "filtered_train_xyz, filtered_y = separate_outliers(\n",
    "    Xs_train * [multipliers['X'], multipliers['Y'], multipliers['Z']],\n",
    "    y_train.ravel(), threshold\n",
    ")[:2] \n",
    "\n",
    "# Compute global min and max values for color scaling\n",
    "#vmin, vmax = np.min(all_values), np.max(all_values)\n",
    "vmin, vmax = np.percentile(all_values, [0.2, 99.8])  # Using 0.2th and 99.8th percentiles to exclude outliers\n",
    "\n",
    "# Create a 3x4 Matplotlib figure with tighter spacing\n",
    "fig, axs = plt.subplots(3, 4, figsize=(24, 15), subplot_kw={'projection': '3d'},\n",
    "                        gridspec_kw={'wspace': 0.4, 'hspace': 0})\n",
    "\n",
    "for ax_row in axs:\n",
    "    for ax in ax_row:\n",
    "        ax.set_facecolor((0, 0, 0, 0)) \n",
    "\n",
    "titles = ['Training data', 'Deep neural net standard deviation', 'Ordinary kriging standard deviation', 'Deepkriging standard deviation']\n",
    "\n",
    "# First column: Training Data with slight rotations\n",
    "rotations = [(30, -60), (30, 350), (30, 280)]  \n",
    "for i, (elev, azim) in enumerate(rotations):\n",
    "    axs[i, 0].scatter(\n",
    "        filtered_train_xyz[:, 0], filtered_train_xyz[:, 1], filtered_train_xyz[:, 2],\n",
    "        color='green', s=8  \n",
    "    )\n",
    "    axs[i, 0].view_init(elev=elev, azim=azim)  # Apply rotation\n",
    "    if i == 0:\n",
    "        axs[i, 0].set_title('Training data', fontsize=12)\n",
    "    axs[i, 0].set_xlabel('Easting [m]', fontsize=8)\n",
    "    axs[i, 0].set_ylabel('Northing [m]', fontsize=8)\n",
    "    axs[i, 0].set_zlabel('Altitude [m]', fontsize=8)\n",
    "    axs[i, 0].tick_params(labelsize=7)\n",
    "    axs[i, 0].grid(False)\n",
    "\n",
    "for i, row_data in enumerate(filtered_data):\n",
    "    for j, entry in enumerate(row_data):\n",
    "        if j == 0:  \n",
    "            (inliers_xyz, inliers_values), (outliers_xyz, outliers_values) = entry\n",
    "            # Plot inliers with color map\n",
    "            axs[i, j + 1].scatter(\n",
    "                inliers_xyz[:, 0], inliers_xyz[:, 1], inliers_xyz[:, 2],\n",
    "                c=inliers_values, cmap=custom_cmap, s=17, vmin=vmin, vmax=vmax\n",
    "            )\n",
    "            # Plot outliers in gray\n",
    "            axs[i, j + 1].scatter(\n",
    "                outliers_xyz[:, 0], outliers_xyz[:, 1], outliers_xyz[:, 2],\n",
    "                color='gray', s=17\n",
    "            )\n",
    "        else:\n",
    "            xyz, values = entry\n",
    "            axs[i, j + 1].scatter(\n",
    "                xyz[:, 0], xyz[:, 1], xyz[:, 2],\n",
    "                c=values, cmap=custom_cmap, s=17, vmin=vmin, vmax=vmax\n",
    "            )\n",
    "        if i == 0:\n",
    "            axs[i, j + 1].set_title(titles[j + 1], fontsize=12)\n",
    "        axs[i, j + 1].set_xlabel('Easting [m]', fontsize=8)\n",
    "        axs[i, j + 1].set_ylabel('Northing [m]', fontsize=8)\n",
    "        axs[i, j + 1].set_zlabel('Altitude [m]', fontsize=8)\n",
    "        axs[i, j + 1].tick_params(labelsize=7)\n",
    "        axs[i, j + 1].grid(False)\n",
    "\n",
    "# Create a shared colorbar with arrows \n",
    "norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "sm = plt.cm.ScalarMappable(cmap=custom_cmap, norm=norm)\n",
    "sm.set_array([])\n",
    "\n",
    "cbar = fig.colorbar(sm, ax=axs, pad=0.05, aspect=50, extend='both', location='right')\n",
    "cbar.set_label('Standard Deviation (g/cm³)', fontsize=10)\n",
    "cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"Figures/uncertainty_plots10.png\", dpi=300, bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geostatistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
